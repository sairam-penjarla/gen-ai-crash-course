{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ.get('TOKEN')\n",
    "print(TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = os.environ.get('TOKEN')\n",
    "HOST = os.environ.get('HOST')\n",
    "MODEL = os.environ.get('MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = TOKEN,\n",
    "    base_url = HOST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "- **`messages`**:  \n",
    "  A list of message objects defining the conversation.\n",
    "\n",
    "- **`model`**:  \n",
    "  The model to use. `gpt-3.5-turbo` is a faster and cheaper version of GPT-4.\n",
    "\n",
    "- **`max_tokens`**:  \n",
    "  Limits the number of tokens (words/pieces of words) in the response.\n",
    "\n",
    "- **`temperature`**:  \n",
    "  Controls randomness in output. Lower values make it deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help with any questions or topics you'd like to discuss! How about you? How's your day going\n"
     ]
    }
   ],
   "source": [
    "# Make a request to the chat model\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKAY! HERE'S ONE: WHY DID THE SCARECROW WIN AN AWARD? BECAUSE HE WAS OUTSTANDING IN HIS FIELD! HAHAHA!\n"
     ]
    }
   ],
   "source": [
    "# Make a request to the chat model\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who always talks in all caps.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a random joke\"},\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=0.99\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE LOS ANGELES DODGERS WON THE WORLD SERIES IN 2020.\n"
     ]
    }
   ],
   "source": [
    "# Make a request to the chat model\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the World Series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say it again, but in all caps.\"},\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Conversations using  a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "                You are an AI assistant who needs to answer user's \n",
    "                question using the provided relevant content\n",
    "                \"\"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "def invoke_llm(user_input):\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"user's question':{user_input}\"\n",
    "    }\n",
    "\n",
    "    conversations.append(msg)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "                        messages=conversations,\n",
    "                        model=MODEL,\n",
    "                        max_tokens=50,\n",
    "                        temperature=0.7\n",
    "                      )\n",
    "\n",
    "    llm_output = chat_completion.choices[0].message.content\n",
    "\n",
    "    conversations.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_output\n",
    "    })\n",
    "\n",
    "    print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I don't have the most up-to-date information on the release year of Grand Theft Auto 6 (GTA 6). As of my knowledge cutoff, there hasn't been an official announcement from Rockstar Games, the developer of the\n"
     ]
    }
   ],
   "source": [
    "invoke_llm(user_input = \"in which year is GTA 6 comming out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No release year is set in stone,\n",
      "For GTA 6, the wait is unknown,\n",
      "Rockstar's silence leaves fans to atone.\n"
     ]
    }
   ],
   "source": [
    "invoke_llm(user_input = \"Say it again in the form of a three line poem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your original question was: \"in which year is GTA 6 coming out\"\n"
     ]
    }
   ],
   "source": [
    "invoke_llm(user_input = \"what was my original question?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Contextual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "                You are an AI assistant who needs to answer user's \n",
    "                question using the provided relevant content\n",
    "                \"\"\"\n",
    "  }\n",
    "]\n",
    "\n",
    "def invoke_llm(content, user_input):\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"relavant content: {content} user's question':{user_input}\"\n",
    "    }\n",
    "\n",
    "    conversations.append(msg)\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "                        messages=conversations,\n",
    "                        model=MODEL,\n",
    "                        max_tokens=256,\n",
    "                        temperature=0.7\n",
    "                      )\n",
    "\n",
    "    llm_output = chat_completion.choices[0].message.content\n",
    "\n",
    "    conversations.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_output\n",
    "    })\n",
    "\n",
    "    return llm_output\n",
    "\n",
    "\n",
    "\n",
    "invoke_llm(\n",
    "    content = \"\"\"\n",
    "            rockstar has released the trailer of GTA 6 on 2024 and has announced \n",
    "            that the game will out out in 2025\n",
    "            \"\"\",\n",
    "    user_input = \"in which year is GTA 6 comming out\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
